\chapter{Focus on data-specific challenges}
While architectural innovations have driven significant progress, the performance of any HMLC model is ultimately constrained by the quality and characteristics of the available data. Real-world datasets for narrative detection and other complex classification tasks are rarely pristine; they are often imbalanced, small, and increasingly, multilingual.

\section{Class imbalance and tail labels}
Perhaps the most pervasive challenge in real-world classification is extreme class imbalance, often described as a long-tailed distribution. In such datasets, a small number of "head" classes are represented by a vast number of training examples, while the majority of "tail" classes are represented by very few, sometimes single-digit, examples. \cite{huang-etal-2021-balancing} This constitutes a severe challenge for standard training algorithms, which, when optimized for overall accuracy, tend to become biased towards the majority classes and perform poorly on the infrequent but often more interesting tail classes.


\textbf{The inadequation of conventional resampling}

In single-label classification, a common strategy to combat imbalance is data resampling, either oversampling the minority classes or undersampling the majority classes \cite{Luque2019The,Thabtah2020Data}. However, this approach is fundamentally ill-suited for the multi-label context. An instance in an MLC dataset can simultaneously belong to multiple classes, for example, one highly frequent head class and one extremely rare tail class. If one were to oversample this instance to increase the representation of the tail class, one would inadvertently also increase the representation of the already-dominant head class, potentially exacerbating the overall imbalance with respect to other labels. This entanglement makes simple resampling ineffective and often counterproductive. \cite{yuan2024research}


A particularly effective technique that has been instancied is the distribution-based loss function. As demonstrated by Huang et al. (2021), this approach goes beyond simple re-weighting by class frequency. It introduces a loss formulation that inherently addresses both the class imbalance and the label linkage (co-occurrence) problems simultaneously. The re-weighting scheme is designed to account for the fact that an instance with multiple labels can be oversampled if each of its labels is treated independently \cite{huang-etal-2021-balancing}.

\section{Strategies for low resource and few-shot scenarios}

When unlabeled data is abundant but labeling is expensive, active learning provides a principled way to build a high-quality training set with minimal human effort \cite{li-etal-2023-enhancing-extreme}. Instead of randomly selecting data for annotation, an active learning system iteratively queries a human oracle (e.g., a domain expert) to label the instances that the model is most uncertain about or that are expected to provide the most information for improving the model. For XMC, Li et al. (2023) propose a sophisticated active learning pipeline that uses a Bi-Encoder model to first retrieve a manageable pool of potentially relevant documents for new, unlabeled classes. A greedy acquisition strategy then selects the most promising candidates from this pool for expert annotation, drastically reducing the labeling burden compared to annotating the entire corpus.\cite{li2024surveyDAL}

\section{Parameter-efficient fine-tuning for low-resource adaptation}

Fully fine-tuning a large PLM with billions of parameters on a small dataset is not only computationally expensive but also prone to overfitting. Parameter-Efficient Fine-Tuning (PEFT) methods have emerged as a critical technology for low-resource adaptation. \cite{su-etal-2024-unlocking} Techniques like Adapters and Low-Rank Adaptation (LoRA) work by freezing the vast majority of the pre-trained model's weights and introducing a very small number of new, trainable parameters. LoRA, for example, injects trainable low-rank matrices into the layers of the Transformer, allowing the model to adapt to the new task by learning low-rank updates to its original weight matrices. By drastically reducing the number of trainable parameters (often by over 99\%), PEFT makes the fine-tuning process more stable on small datasets, reduces memory requirements, and helps prevent catastrophic forgetting of the knowledge learned during pre-training.~\cite{liu2024alora,razuvayevskaya2024comparison}

\section{Data augmentation with LLMs}
As discussed in Section~\ref{sec:llm-augmentation}, LLMs can serve as powerful data generators. For low-resource classes, an LLM can be prompted to create synthetic training examples by paraphrasing the few existing seed examples or by generating novel instances based on a class description. A cost-benefit analysis by Cegin et al. (2025) reveals that this approach provides the most significant advantage when the number of initial seed examples is extremely small (e.g., one-shot or few-shot learning) \cite{cegin-etal-2025-llms}. As the amount of available data increases, the performance gains from LLM augmentation may diminish relative to its computational cost, and in some cases, traditional augmentation techniques can achieve comparable results more efficiently.
