\chapter{Foundational paradigms of MLC}
The main challenge in multi-label classification (MLC) was how to adapt algorithms designed for single-label (binary or multiclass) problems to handle multiple labels per instance. The core issue these methods faced was the presence of label correlations: in real-world data, labels are often not independent. For example, a news article tagged "Politics" is much more likely to also be tagged "Elections" than "Sports." Treating each label as a separate binary problem ignores these dependencies, potentially leading to suboptimal predictions.

Classical MLC methods can be seen as different strategies for balancing computational simplicity with the need to model label correlations. Some methods, like Binary Relevance, treat each label independently for simplicity, but this can miss important relationships between labels. Others, such as Classifier Chains or Label Powerset, explicitly model these correlations, but at the cost of increased computational complexity. The choice of method reflects a trade-off between efficiency and the ability to capture the true structure of the data.

To address this, early research focused on a family of techniques known as problem transformation methods, which decompose the multi-label task into one or more single-label problems. These methods are algorithm-independent, allowing any standard classifier to be applied. The three canonical approaches represent distinct strategies for managing label dependencies.

\section{Binary Relevance}
Binary Relevance (BR) is the most intuitive approach. It decomposes the MLC problem with a label set of size $|\mathcal{L}|$ into independent binary classification problems. For each label, a separate classifier is trained to predict its presence or absence, effectively ignoring all other labels. \cite{zhang_binary_2018} The primary advantage of BR is its simplicity and efficiency, as the classifiers can be trained in parallel. Its main drawback, however, is the foundational label independence assumption, which completely disregards label correlations and can lead to lower predictive accuracy and logically incoherent label set predictions. \cite{Sucar2014Multi-label}

\section{Label Powerset}
In contrast to decompositional methods, the Label Powerset (LP) method reframes the entire problem at once. It converts the multi-label task into a standard multi-class problem by mapping each distinct set of co-occurring labels to a single, unique class. For instance, the label sets `{Politics, Elections}` and `{Sports, Weather}` would become two separate classes for a multi-class classifier to learn. \cite{Read2011}

The main advantage of this approach is its ability to perfectly model the dependencies between labels for all combinations it has seen, as these correlations are baked into the class definitions \cite{Sucar2014Multi-label}. However, this strategy is often impractical. The number of potential classes can become unmanageably large as the label set grows, a problem known as combinatorial explosion. This leads to a highly sparse class distribution where many label sets appear only a few times, making it difficult to train a robust model \cite{Cherman2011}. Critically, the LP method cannot generalize to predict any combination of labels that did not appear in the training data.

\section{Classifier Chains}
The Classifier Chains (CC) method was proposed as a novel approach to overcome the stark trade-off between the label-independent BR and the computationally explosive LP. It seeks to model label dependencies while maintaining the efficiency of a binary relevance framework \cite{Read2011}.

Like BR, CC trains $|\mathcal{L}|$ binary classifiers. However, instead of being independent, these classifiers are linked in a chain. The first classifier in the chain, $\mathcal{C}_1$, predicts the presence or absence of the first label. The predictions of this classifier are then used as additional features for the second classifier, $\mathcal{C}_2$, which predicts the second label. This process continues down the chain, with each classifier potentially benefiting from the predictions of all previous classifiers. This allows CC to capture label dependencies while still being relatively efficient to train. \cite{Read2011} The order of the chain can significantly impact performance, as earlier classifiers influence later ones \cite{Read2021}. To mitigate this, ensemble methods that average predictions over multiple random chain orders are often employed \cite{Sucar2014Multi-label,Zhang2018}.

