\documentclass[12pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
% Allow tables to span multiple pages when necessary
\usepackage{longtable}
\usepackage{booktabs}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.


\title{Instructions for *ACL Proceedings}

\author{Nour Eljadiri \\
  University of Passau \\
  INSA Lyon \\
  \texttt{mohamed.eljadiri@insa-lyon.fr} \\
}

\begin{document}

\maketitle

\begin{abstract}
    The task of assigning multiple, hierarchically-structured labels to text documents, known as Hierarchical Multi-Label Classification (HMLC), is critical in domains from scientific archiving to legal analysis. This review traces the methodological evolution of HMLC, beginning with foundational problem transformation methods like Binary Relevance and Classifier Chains, which primarily address the challenge of label correlation. We then examine the paradigm shift introduced by pre-trained Transformers, dissecting the dichotomy between local, top-down approaches prone to error propagation and global, hierarchy-aware models that integrate structural constraints via specialized loss functions or Graph Neural Networks. Finally, we explore the current frontier, where Large Language Models (LLMs) are reframing the task through generative paradigms, enabled by techniques such as LLM-powered data augmentation, instruction fine-tuning, and Parameter-Efficient Fine-Tuning (PEFT). This narrative highlights a progression towards increasingly sophisticated methods for embedding hierarchical prior knowledge into statistical models.
\end{abstract}

\input{introduction}

\input{foundational_paradigms}

\input{hierarchy_aware_architectures}

\input{large_language_models}

\input{data_specific_challenges}

\input{conclusion}
\bibliography{references, anthology}
\end{document}
