\section{Introduction}

Text Classification (TC) stands as one of the most foundational and widely researched tasks within the domain of Natural Language Processing (NLP). \cite{Zangari2024}  In its most common formulation, TC involves supervised learning algorithms designed to map a given piece of text, or document, to a predefined set of labels or categories. \cite{Zangari2024} Historically, this has often been a multiclass problem, where each document is assigned to exactly one class from a set of mutually exclusive options. However, the increasing complexity and richness of information in modern digital text have rendered this single-label paradigm insufficient.\cite{Hu2025}  Many documents, from news articles and scientific papers to legal filings and product descriptions, simultaneously encompass multiple topics or themes. \cite{TidakeSane2018}

This reality gave rise to Multi-Label classification (MLC), a more challenging variant of text classification where each data sample can be associated with one or multiple labels simultaneously.\cite{TidakeSane2018} The core challenge in MLC, which distinguishes it from simply running multiple independent binary classifiers, is the presence of correlations between labels \cite{TidakeSane2018}, that is, the assignment of one label often provides strong statistical evidence for or against the assignment of another, and effectively modeling these inter-label dependencies has become a central focus of research in the field. \cite{Huang2024, TidakeSane2018}

Formally, in MLC, the goal is to learn a function $f: X \rightarrow 2^L$ that maps an instance $x \in X$ to a subset of labels $Y \subseteq L$, where $L = \{l_1, l_2, \ldots, l_L\}$ is the finite set of all possible labels. The number of labels associated with an instance is not fixed and can vary. \cite{TidakeSane2018}

Hierarchical Multi-Label Text Classification (HMLC), the primary subject of this review, introduces a further layer of complexity and structure to the MLC problem. HMLC is defined as a classification task where instances may not only belong to multiple classes simultaneously, but where these classes are themselves organized within a predefined hierarchy \cite{liu2023recentadvanceshierarchicalmultilabel}. This hierarchical structure, typically represented as a tree or a Directed Acyclic Graph (DAG), formalizes the relationships among the labels, arranging them from broader, coarse-grained categories at higher levels to more specific, fine-grained ones. \cite{liu2023recentadvanceshierarchicalmultilabel}

This structured approach is particularly relevant for analyzing the sophisticated communication strategies found in online media. The rapid spread of online news has increased exposure to deceptive narratives and manipulation attempts, especially during major crisis events like geopolitical conflicts. To support research in this area, tasks such as SemEval-2025 Task 10 have been established, focusing on automated narrative classification \cite{semeval2025task10}. The goal is to categorize news articles by assigning them multiple labels from a two-level taxonomy of predefined narratives and subnarratives. Addressing this HMLC problem requires models that can understand nuanced content while respecting the explicit hierarchical dependencies between labels.

The task of narrative detection in text is an ideal application for HMLC. Narratives are structured frameworks of meaning that shape the interpretation of events and issues. A single text can invoke multiple, often nested, narratives. For example, a news report on an international incident might simultaneously employ a broad "National Security" narrative, a more specific "Foreign Aggression" sub-narrative, and a granular "Economic Sanctions" micro-narrative. An HMLC framework can model this structure, capturing both the multiple narrative elements present and their hierarchical relationships.

This review will trace the methodological evolution of HMLC. We begin by examining foundational problem transformation techniques such as Binary Relevance \cite{zhang_binary_2018}, Classifier Chains \cite{li_relative_2024, weng_label_2020}, and the Label Powerset method \cite{shan_co-learning_2018}. We then transition to the current state-of-the-art, dominated by deep learning models leveraging Transformer architectures like BERT \cite{devlin_bert_2019} and its multilingual variants such as XLM-RoBERTa \cite{conneau_unsupervised_2020}. Finally, we will cover specialized strategies like hierarchical classification models \cite{sadat_hierarchical_2022} and graph-based methods \cite{peng_hierarchical_2021, gong_hierarchical_2020} designed to explicitly model the structured taxonomies inherent to HMLC.